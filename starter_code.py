# -*- coding: utf-8 -*-
"""Starter_Code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XUicXaI1u9y2vs8kivwnXjzYEljhPak-

## Preprocessing
"""

# Import our dependencies
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import pandas as pd
import tensorflow as tf

#  Import and read the charity_data.csv.
import pandas as pd
application_df = pd.read_csv("https://static.bc-edx.com/data/dl-1-2/m21/lms/starter/charity_data.csv")
application_df.head()

# Drop the non-beneficial ID columns, 'EIN' and 'NAME'.
application_df = application_df.drop(columns=['EIN', 'NAME'])

# Determine the number of unique values in each column.
unique_counts = application_df.nunique()
print(unique_counts)

# Look at APPLICATION_TYPE value counts to identify and replace with "Other"
app_type_counts = application_df['APPLICATION_TYPE'].value_counts()
print(app_type_counts)

# Choose a cutoff value and create a list of application types to be replaced
# use the variable name `application_types_to_replace`
cutoff =
application_types_to_replace = app_type_counts[app_type_counts < cutoff].index

# Replace in dataframe
for app in application_types_to_replace:
    application_df['APPLICATION_TYPE'] = application_df['APPLICATION_TYPE'].replace(app,"Other")

# Check to make sure replacement was successful
application_df['APPLICATION_TYPE'].value_counts()

# Look at CLASSIFICATION value counts to identify and replace with "Other"
classification_counts = application_df['CLASSIFICATION'].value_counts()
print(classification_counts)

# You may find it helpful to look at CLASSIFICATION value counts >1
classification_counts_gt_1 = classification_counts[classification_counts > 1]
print(classification_counts_gt_1)

# Choose a cutoff value and create a list of classifications to be replaced
# use the variable name `classifications_to_replace`
classifications_to_replace = classification_counts[classification_counts < cutoff].index

# Replace in dataframe
for cls in classifications_to_replace:
    application_df['CLASSIFICATION'] = application_df['CLASSIFICATION'].replace(cls,"Other")

# Check to make sure replacement was successful
application_df['CLASSIFICATION'].value_counts()

# Convert categorical data to numeric with `pd.get_dummies`
application_df_encoded = pd.get_dummies(application_df)

# Display the first few rows of the encoded DataFrame to check the result
application_df_encoded.head()

# Split our preprocessed data into our features and target arrays
X = application_df_encoded.drop('IS_SUCCESSFUL', axis=1)
y = application_df_encoded['IS_SUCCESSFUL']


# Split the preprocessed data into a training and testing dataset
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Display the shapes of the resulting datasets
print(f"X_train shape: {X_train.shape}")
print(f"X_test shape: {X_test.shape}")
print(f"y_train shape: {y_train.shape}")
print(f"y_test shape: {y_test.shape}")

# Create a StandardScaler instances
scaler = StandardScaler()

# Fit the StandardScaler
X_scaler = scaler.fit(X_train)

# Scale the data
X_train_scaled = X_scaler.transform(X_train)
X_test_scaled = X_scaler.transform(X_test)

# Display the shapes of the scaled datasets to confirm the scaling
print(f"X_train_scaled shape: {X_train_scaled.shape}")
print(f"X_test_scaled shape: {X_test_scaled.shape}")

"""## Compile, Train and Evaluate the Model"""

# Define the model - deep neural net, i.e., the number of input features and hidden nodes for each layer.
input_dim = X_train_scaled.shape[1]  # Number of features in the input layer

# Initialize the model
nn = tf.keras.models.Sequential()

# First hidden layer
nn.add(tf.keras.layers.Dense(units=8, activation='relu', input_dim=input_dim))

# Second hidden layer
nn.add(tf.keras.layers.Dense(units=8, activation='relu'))

# Output layer
nn.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))

# Check the structure of the model
nn.summary()

# Compile the model
nn.compile(optimizer='adam',
           loss='binary_crossentropy',
           metrics=['accuracy'])

# Check that the model has been compiled correctly
print("Model compiled successfully!")

# Train the model
history = nn.fit(X_train_scaled,
                 y_train,
                 epochs=100,
                 batch_size=32,
                 validation_split=0.2,
                 verbose=1)

# Check that training is complete
print("Model training complete!")

# Evaluate the model using the test data
model_loss, model_accuracy = nn.evaluate(X_test_scaled, y_test, verbose=2)

# Print the loss and accuracy
print(f"Loss: {model_loss}, Accuracy: {model_accuracy}")

# Export our model to an HDF5 file
nn.save('AlphabetSoupCharity.h5')

# Confirm that the model was saved
print("Model saved to AlphabetSoupCharity.h5")

# Load the model from the HDF5 file
loaded_model = tf.keras.models.load_model('AlphabetSoupCharity.h5')

# Print the model summary to inspect its architecture
loaded_model.summary()

import h5py

# Open the HDF5 file
with h5py.File('AlphabetSoupCharity.h5', 'r') as file:
    # List all root level object names (aka keys)
    print("Keys: %s" % file.keys())
    a_group_key = list(file.keys())[0]

    # Get the HDF5 group object for the first key
    group = file[a_group_key]

    # Print the keys of this group
    print("Group keys: %s" % group.keys())

    # Print the contents of each key
    for key in group.keys():
        print(f"{key}: {group[key]}")